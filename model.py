import torch

# 8-layer, 8-head transformer
# embedding dim 256
# SwiGLU activation
# learnable positional embeddings
# scaled dot product attention
